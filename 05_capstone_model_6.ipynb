{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd064e7fe895b7a30084ff24eba202738ac5aa85c3b47075d056ba99cbf74e6a593",
   "display_name": "Python 3.8.5 64-bit ('.venv': venv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Model for outcome 6 months after the (first) stroke"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functions import modeling_14d\n",
    "\n",
    "# plot libarys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MLflow\n",
    "import sys\n",
    "# adding to the path variables the one folder higher (locally, not changing system variables)\n",
    "sys.path.append(\"..\")\n",
    "import mlflow\n",
    "from modeling.config import TRACKING_URI, EXPERIMENT_NAME\n",
    "\n",
    "# Model preparation\n",
    "#import imblearn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_validate\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    " \n",
    "\n",
    "# Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz  \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model Metrics\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix, classification_report, plot_confusion_matrix\n",
    "from sklearn.metrics import f1_score, make_scorer # for modelling.py\n",
    "\n",
    "# for merging the dataframes\n",
    "import os, glob\n",
    "import json\n",
    "\n",
    "# further libarys\n",
    "#import itertools\n",
    "from sklearn.tree import export_graphviz\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/IST_corrected_model.csv',  index_col= [0])"
   ]
  },
  {
   "source": [
    "### Target for 14 days after first stroke"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Target for the second model is the outcome 6 months after the (first) stroke. (OCCODE)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dependent        7088\n",
       "Dead             3611\n",
       "Not recovered    3537\n",
       "Recovered        2988\n",
       "9                  47\n",
       "Name: OCCODE, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.OCCODE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "features = ['SEX', 'AGE', 'RSBP', 'STYPE']\n",
    "X = df[features]\n",
    "\n",
    "# Select target\n",
    "y = df.ALIVE14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0.0, 1685), (1.0, 15586)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(Counter(y).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Patients deceased: 9.8 %\nPatients alive: 90.2 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Patients deceased: {round((df.ALIVE14.value_counts()[0] / df.shape[0] * 100),1)} %')\n",
    "print(f'Patients alive: {round((df.ALIVE14.value_counts()[1] / df.shape[0] * 100),1)} %')"
   ]
  },
  {
   "source": [
    "We have an imbalanced dataset - with a distribution of 90% alive and 10% deceased patients after an ischaemic stroke. Below "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in test and training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "#### Helper functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_eval_plot_model(X_train, X_test, y_train, y_test, clf, cv=None):\n",
    "    \"\"\"Train a single model and print evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame, np.array): Features of the training set\n",
    "        X_test (pd.DataFrame, np.array): Features of thee test set\n",
    "        y_train (pd.Series, np.array): Target of the training set\n",
    "        y_teset (pd.Seeries, np.array): Target of the test set\n",
    "        clf (sklearn.base.BaseEstimator): Estimator to train and use\n",
    "        cv (int, None): Number of cross-validations, default=None\n",
    "    \n",
    "    Returns:\n",
    "        model (sklearn.base.BaseEstimator): The trained model\n",
    "    \"\"\"\n",
    "    model = clf.fit(X_train, y_train)\n",
    "\n",
    "    if cv:\n",
    "        cv = cross_validate(m_rf, X_train_trans, y_train, cv=5, verbose=5)\n",
    "        print(f\"Best cross-validated score: {cv['test_score'].mean()}\")\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"--- MODEL PARAMETERS {'-'*10}\")\n",
    "    print(json.dumps(model.get_params(), indent=4))\n",
    "    print(f\"--- CLASSIFICATION REPORT {'-'*10}\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(f\"--- CONFUSION MATRIX {'-'*10}\")\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    plot_confusion_matrix(model, X_test, y_test, display_labels=[\"deceased\", \"alive\"])\n",
    "    return model\n",
    "\n",
    "def _pred_eval_plot_grid(X_train, X_test, y_train, y_test, gs):\n",
    "    \"\"\"Helper function to perform a grid search and calculate performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame, np.array): Features of the training set\n",
    "        X_test (pd.DataFrame, np.array): Features of thee test set\n",
    "        y_train (pd.Series, np.array): Target of the training set\n",
    "        y_teset (pd.Seeries, np.array): Target of the test set\n",
    "        gs (BaseSearchCV): SearchCV to train and use\n",
    "    \n",
    "    Returns:\n",
    "        model (BaseSearchCV): The trained grid search\n",
    "    \"\"\"\n",
    "    gs = gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Testing predictions (to determine performance)\n",
    "    y_pred = gs.best_estimator_.predict(X_test)\n",
    "    \n",
    "    print(f\"--- GRID SEARCH RESULTS {'-'*10}\")\n",
    "    print(f\"Best model: {gs.best_params_}\")\n",
    "    print(f\"Best cross-validated score: {gs.best_score_}\")\n",
    "    print(f\"--- CLASSIFICATION REPORT {'-'*10}\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(f\"--- CONFUSION MATRIX {'-'*10}\")\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    plot_confusion_matrix(gs.best_estimator_, X_test, y_test, y_test, display_labels=[\"deceased\", \"alive\"])\n",
    "    return gs\n",
    "    \n",
    "\n",
    "def run_rand_grid_search(X_train, X_test, y_train, y_test, clf, params_grid, n_iter=10, cv=5):\n",
    "    \"\"\"Perform a randomized grid search and calculate performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame, np.array): Features of the training set\n",
    "        X_test (pd.DataFrame, np.array): Features of thee test set\n",
    "        y_train (pd.Series, np.array): Target of the training set\n",
    "        y_teset (pd.Seeries, np.array): Target of the test set\n",
    "        clf (sklearn.base.BaseEstimator): Estimator to train and use\n",
    "        params_grid (dict): Dictionary defining the parameters for the grid search\n",
    "        n_iter (int): Number of grid search combinations to run\n",
    "        cv (int, None): Number of cross-validations, default=None\n",
    "        \n",
    "    Returns:\n",
    "        model (BaseSearchCV): The trained grid search\n",
    "    \"\"\"\n",
    "    gs = RandomizedSearchCV(clf, params_grid, n_iter=n_iter, cv=cv, random_state=24, verbose=5)\n",
    "    return _pred_eval_plot_grid(X_train, X_test, y_train, y_test, gs)\n",
    "    \n",
    "def run_grid_search(X_train, X_test, y_train, y_test, clf, params_grid, cv=5):\n",
    "    \"\"\"Perform a grid search and calculate performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame, np.array): Features of the training set\n",
    "        X_test (pd.DataFrame, np.array): Features of thee test set\n",
    "        y_train (pd.Series, np.array): Target of the training set\n",
    "        y_teset (pd.Seeries, np.array): Target of the test set\n",
    "        clf (sklearn.base.BaseEstimator): Estimator to train and use\n",
    "        params_grid (dict): Dictionary defining the parameters for the grid search\n",
    "        cv (int, None): Number of cross-validations, default=None\n",
    "        \n",
    "    Returns:\n",
    "        model (BaseSearchCV): The trained grid search\n",
    "    \"\"\"\n",
    "    gs = GridSearchCV(clf, params_grid, cv=cv, verbose=5)\n",
    "    return _pred_eval_plot_grid(X_train, X_test, y_train, y_test, gs)\n",
    "    "
   ]
  },
  {
   "source": [
    "### Preparation of Data Scaling and Category Encoding\n",
    "Some models need scaling of numerical features and encoding of categorical features. The sklearn preprocessors are instantiated here and used where necessary in the data transformation step of each model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder for categories\n",
    "onehot = OneHotEncoder(drop=\"first\")\n",
    "\n",
    "# scalers for numerical features\n",
    "mms = MinMaxScaler()\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Prepare list of numerical and categorical columns\n",
    "num_cols = make_column_selector(dtype_include=np.number)\n",
    "cat_cols = make_column_selector(dtype_include=\"object\")"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}